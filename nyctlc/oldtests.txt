 #@pytest.mark.parametrize("input_url, expected_row_count", [
    #    ("https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-10.parquet1", 3675411),
        #("https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-10.parquet", 1),
        #("https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2022-10.parquet", 1),
        #("https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-10.parquet", 1)
    #])
def test_get_remote_parquet(self, input_url, expected_row_count):
        """
        This method tests the download_source method.

        Parameters:
            input_url (str): The URL to download the data from.
            expected_row_count (int): The expected number of rows in the dataframe.
        """
        #init actual_count to 0
        actual_count = 0
        try:
            test_ingest = ingest.Ingest()
            actual = test_ingest.get_remote_parquet(input_url)
            actual_count = actual.count()


        except AttributeError:
            assert actual_count == 0

        finally:
            assert actual_count == expected_row_count



 def get_remote_parquet(self, url):
        """
        This method reads remote parquet files into a dataframe.

        Parameters:
            url (str): The URL to download the data from.
            timeout (int): The number of seconds to wait before timing out the request.

        """
        try:
            self.spark.sparkContext.addFile(url)
            file_name = url.split("/")[-1]
            return self.spark.read.parquet(SparkFiles.get(file_name))

        except BaseException as exception: #pylint: disable=broad-except
            print(exception)
            return None
